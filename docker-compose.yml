version: "3"
services:
  hadoop-slave1:
    build:
      context: hadoop/.
      shm_size: '2gb'
    container_name: hadoop-slave1
    networks:
      default:
         ipv4_address: 172.16.0.3
    extra_hosts:
      - "hadoop-master: 172.16.0.2"
      - "hadoop-slave2: 172.16.0.4"
    command: bash -c  " 
         hadoop-daemon.sh --config /usr/local/hadoop/etc/hadoop start datanode
         && yarn-daemon.sh --config /usr/local/hadoop/etc/hadoop start nodemanager
         && tail -f /dev/null"
    hostname: hadoop-slave1
    restart: always
  hadoop-slave2:
    build: 
      context: hadoop/.
      shm_size: '2gb'
    container_name: hadoop-slave2
    networks:
      default:
         ipv4_address: 172.16.0.4
    extra_hosts:
        - "hadoop-master: 172.16.0.2"
        - "hadoop-slave1: 172.16.0.3"
    command: bash -c  " 
         hadoop-daemon.sh --config /usr/local/hadoop/etc/hadoop start datanode
         && yarn-daemon.sh --config /usr/local/hadoop/etc/hadoop start nodemanager
         && tail -f /dev/null"
    hostname: hadoop-slave2
    restart: always
  hadoop-master:
    build:
      context: hadoop/.
      shm_size: '2gb'
      args: 
           FORMAT_NAMENODE_COMMAND: hdfs namenode -format
    container_name: hadoop-master
    networks:
      default:
         ipv4_address: 172.16.0.2
    extra_hosts:
      - "hadoop-slave1: 172.16.0.3"
      - "hadoop-slave2: 172.16.0.4"
    command: bash -c  "
        start-dfs.sh
        && start-yarn.sh
        && mr-jobhistory-daemon.sh start historyserver
        && tail -f /dev/null"
    ports:
      - 50070:50070
      - 8088:8088
    hostname: hadoop-master
    restart: always
  spark-master:
    build:
      context: spark/.
      shm_size: '2gb'
    container_name: spark-master
    networks:
      default:
        ipv4_address: 172.16.0.5
    extra_hosts:
      - "spark-worker: 172.16.0.6"
      - "hadoop-master: 172.16.0.2"
    command: bash -c "
        start-master.sh
        && tail -f /dev/null"
    ports:
      - 8080:8080
      - 4040:4040
    hostname: spark-master
    restart: always
  spark-worker:
    build:
      context: spark/.
      shm_size: '2gb'
    container_name: spark-worker
    networks:
      default:
        ipv4_address: 172.16.0.6
    extra_hosts:
      - "spark-master: 172.16.0.5"
    command: bash -c "
        start-slave.sh spark://spark-master:7077
        && tail -f /dev/null"
    hostname: spark-worker
    restart: always
  hive:
    depends_on:
      - "hadoop-master"
    build:
      context: hive/.
      shm_size: '2gb'
    container_name: hive
    networks:
      default:
        ipv4_address: 172.16.0.7
    extra_hosts:
      - "hadoop-master: 172.16.0.2"
      - "spark-master: 172.16.0.5"
    command: bash -c "
        tail -f /dev/null"
    hostname: hive
    restart: always

networks:
  default:
    external:
      name: hadoop-network

